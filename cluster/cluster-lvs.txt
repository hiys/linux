

部署LVS-NAT集群
集群对外公网IP地址为192.168.0.11
调度器内网IP地址为192.168.1.11
真实Web服务器地址分别为192.168.1.12 、 192.168.1.13
使用加权轮询调度算法,真实服务器权重分别为1和2

client  V10  192.168.0.10
proxy   V11  192.168.0.11  192.168.1.11
web1    V12  192.168.1.12
web2    V13  192.168.1.13


[root@V10 ~]# ifdown  eth1

[root@V10 ~]# ifdown  eth2

[root@V10 ~]# ifconfig |grep  -A1 flags=
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.0.10  netmask 255.255.255.0  broadcast 192.168.0.255
--
lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
--
virbr0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        inet 192.168.122.1  netmask 255.255.255.0  broadcast 192.168.122.255

[root@V10 ~]# route  -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.0.254   0.0.0.0         UG    0      0        0 eth0
169.254.0.0     0.0.0.0         255.255.0.0     U     1002   0        0 eth0
192.168.0.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0
192.168.122.0   0.0.0.0         255.255.255.0   U     0      0        0 virbr0

[root@V10 ~]# tail   -4  /etc/sysconfig/network-scripts/ifcfg-eth0
BOOTPROTO="static"
IPADDR="192.168.0.10"
NETMASK="255.255.255.0"
GATEWAY="192.168.0.254"

[root@V10 ~]# yum  -y install  elinks |tail -1
完毕！
[root@V10 ~]# elinks  -dump  192.168.1.12
ELinks: 拒绝连接

[root@V10 ~]# elinks  -dump  192.168.0.11
   web2 V13 192.168.1.13

[root@V10 ~]# elinks  -dump  192.168.0.11
   web1 V12 192.168.1.12

[root@V10 ~]# elinks  -dump  192.168.0.11
   web2 V13 192.168.1.13
[root@V10 ~]# elinks  -dump  192.168.0.11
   web2 V13 192.168.1.13

[root@V10 ~]# elinks  -dump  192.168.0.11
   web1 V12 192.168.1.12

[root@V10 ~]# elinks  -dump  192.168.0.11
   web2 V13 192.168.1.13
[root@V10 ~]# elinks  -dump  192.168.0.11
   web2 V13 192.168.1.13

[root@V10 ~]# 
  -> RemoteAddress:Port     Forward Weight ActiveConn InActConn
   ->       远程地址：端口                 权重   
TCP  192.168.0.11:80   wrr        
  -> 192.168.1.12:80        Masq    1      0    0         
  -> 192.168.1.13:80        Masq    2      0    0   














部署LVS-NAT集群

client  V10  192.168.0.10
proxy   V11  192.168.0.11  192.168.1.11
web1    V12  192.168.1.12
web2    V13  192.168.1.13


[root@V11 ~]# ifconfig |grep  -A1 flags=
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.0.11  netmask 255.255.255.0  broadcast 192.168.0.255
--
eth1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.1.11  netmask 255.255.255.0  broadcast 192.168.1.255
--
eth2: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.2.11  netmask 255.255.255.0  broadcast 192.168.2.255
--
lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
--
virbr0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        inet 192.168.122.1  netmask 255.255.255.0  broadcast 192.168.122.255

[root@V11 ~]# route  -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.0.254   0.0.0.0         UG    0      0        0 eth0
169.254.0.0     0.0.0.0         255.255.0.0     U     1002   0        0 eth0
169.254.0.0     0.0.0.0         255.255.0.0     U     1003   0        0 eth1
169.254.0.0     0.0.0.0         255.255.0.0     U     1004   0        0 eth2
192.168.0.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0
192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 eth1
192.168.2.0     0.0.0.0         255.255.255.0   U     0      0        0 eth2
192.168.122.0   0.0.0.0         255.255.255.0   U     0      0        0 virbr0

部署LVS-NAT模式调度器
----------------------  确认调度器的路由转发功能 -----------------
[root@V11 ~]# cat   /proc/sys/net/ipv4/ip_forward
1
[root@V11 ~]# vim  /etc/sysctl.conf 

[root@V11 ~]# tail  -1  /etc/sysctl.conf
net.ipv4.ip_forward = 1

[root@V11 ~]# sysctl  -p  /etc/sysctl.conf
net.ipv4.ip_forward = 1

[root@V11 ~]# sysctl -a |grep  net.ipv4.ip_forward
net.ipv4.ip_forward = 1
net.ipv4.ip_forward_use_pmtu = 0
.............

[root@V11 ~]# yum  -y install  ipvsadm |tail  -3
  ipvsadm.x86_64 0:1.27-7.el7                                                   

完毕！
[root@V11 ~]# rpm -qa  |grep ipvsadm
ipvsadm-1.27-7.el7.x86_64

[root@V11 ~]# man  ipvsadm

ipvsadm  -L|-l –list 显示内核虚拟服务器表
         -A –add-service 在内核的虚拟服务器表中添加一条新的虚拟服务器记录。
                                 也就是增加一台新的虚拟服务器。
  -E, --edit-service
       Edit a virtual service
-E --edit-service 编辑内核虚拟服务器表中的一条虚拟服务器记录

 -D, --delete-service
     Delete a virtual service, along with any associated real servers.
-D --delete-service 删除内核虚拟服务器表中的一条虚拟服务器记录

 -C, --clear
          Clear the virtual server table
-C  --clear 清除内核虚拟服务器表中的所有记录。

 -R, --restore   从stdin还原Linux虚拟服务器规则。
             从stdin读取的每一行将被视为单独调用ipvsadm的命令行选项。
             从stdin读取的行可以选择以“ipvsadm”开头。
             此选项有助于避免在构建扩展路由表时执行大量或ipvsadm命令。
-R –restore 恢复虚拟服务器规则

-S, --save   将Linux虚拟服务器规则以 -R_--restore可以读取的格式  保存虚拟服务器规则，
                 输出为-R 选项可读的格式
 ipvsadm -S [-n]

-Z --zero 虚拟服务表计数器清零（清空当前的连接数量等）

-t –tcp-service service-address 说明虚拟服务器提供的是tcp 的服务

-s –scheduler scheduler 使用的调度算法，有这样几个选项
               rr|wrr|lc|wlc|lblc|lblcr|dh|sh|sed|nq,
默认的调度算法是： wlc.
LVS的调度算法分为静态与动态两类。

1.静态算法（4种）：
只根据算法进行调度 而不考虑后端服务器的实际连接情况和负载情况

①.RR：轮叫调度（Round Robin）
　 调度器通过”轮叫”调度算法
将外部请求按顺序轮流分配到集群中的真实服务器上，
它均等地对待每一台服务器，
而不管服务器上实际的连接数和系统负载｡

②.WRR：加权轮叫（Weight RR）
　 调度器通过“加权轮叫”调度算法
根据真实服务器的不同处理能力来调度访问请求。
这样可以保证处理能力强的服务器处理更多的访问流量。
调度器可以自动问询真实服务器的负载情况,
并动态地调整其权值。

③.DH：目标地址散列调度（Destination Hash ）
　 根据请求的目标IP地址，
作为散列键(HashKey)从静态分配的散列表找出对应的服务器，
若该服务器是可用的且未超载，
将请求发送到该服务器，否则返回空。

④.SH：源地址 hash（Source Hash）
　 源地址散列”调度算法根据请求的源IP地址，
作为散列键(HashKey)从静态分配的散列表找出对应的服务器，
若该服务器是可用的且未超载，
将请求发送到该服务器，否则返回空｡

2.动态算法（6种）：
前端的调度器会根据后端真实服务器的实际连接情况来分配请求

①.LC：最少链接（Least Connections）
　 调度器通过”最少连接”调度算法动态地将网络请求调度到已建立的链接数最少的服务器上。
如果集群系统的真实服务器具有相近的系统性能，
采用”最小连接”调度算法可以较好地均衡负载。

②.WLC：加权最少连接(默认采用的就是这种)（Weighted Least Connections）
　 在集群系统中的服务器性能差异较大的情况下，
调度器采用“加权最少链接”调度算法优化负载均衡性能，
具有较高权值的服务器将承受较大比例的活动连接负载｡
调度器可以自动问询真实服务器的负载情况,
并动态地调整其权值。

③.SED：最短延迟调度（Shortest Expected Delay ）
　 在WLC基础上改进，Overhead = （ACTIVE+1）*256/加权，
不再考虑非活动状态，
把当前处于活动状态的数目+1 来实现，
数目最小的，接受下次请求，
 +1 的目的是为了考虑加权的时候，非活动连接过多缺陷：
当权限过大的时候，会倒置空闲服务器一直处于无连接状态。

④.NQ永不排队/最少队列调度（Never Queue Scheduling NQ）
　 无需队列。
如果有台 realserver的连接数＝0就直接分配过去，
不需要再进行sed运算，
保证不会有一个主机很空闲。
在SED基础上无论 +几，第二次一定给下一个，保证不会有一个主机不会很空闲着，
不考虑非活动连接，才用NQ，
SED要考虑活动状态连接，
对于DNS的UDP不需要考虑非活动连接，
而httpd的处于保持状态的服务就需要考虑非活动连接给服务器的压力。

⑤.LBLC：基于局部性的最少链接（locality-Based Least Connections）
　 基于局部性的最少链接”调度算法是针对目标IP地址的负载均衡，
目前主要用于Cache集群系统｡
该算法根据请求的目标IP地址找出该目标IP地址最近使用的服务器，
若该服务器是可用的且没有超载，
将请求发送到该服务器;若服务器不存在，
或者该服务器超载且有服务器处于一半的工作负载，
则用“最少链接”的原则选出一个可用的服务器，将请求发送到该服务器｡

⑥. LBLCR：
带复制的基于局部性最少连接
（Locality-Based Least Connections with Replication）
　 带复制的基于局部性最少链接”调度算法也是针对目标IP地址的负载均衡，
目前主要用于Cache集群系统｡
它与LBLC算法的不同之处是
它要维护从一个目标IP地址到一组服务器的映射，
而LBLC算法维护
从一个目标IP地址到一台服务器的映射｡
该算法根据请求的目标IP地址找出该目标IP地址对应的服务器组，
按”最小连接”原则从服务器组中选出一台服务器，
若服务器没有超载，将请求发送到该服务器；
若服务器超载，则按“最小连接”原则从这个集群中选出一台服务器，
将该服务器加入到服务器组中，将请求发送到该服务器｡
同时，当该服务器组有一段时间没有被修改，
将最忙的服务器从服务器组中删除，
以降低复制的程度。

 -a, --add-server
         Add a real server to a virtual service.
-a  --add-server 在内核虚拟服务器表的一条记录里添加一条新的真实服务器

-e  --edit-server 编辑一条虚拟服务器记录中的某条真实服务器记录

-d –delete-server 删除一条虚拟服务器记录中的某条真实服务器记录

 -e, --edit-server
              Edit a real server in a virtual service.

 -d, --delete-server
              Remove a real server from a virtual service.

-r –real-server server-address 真实的服务器[Real-Server:port]

-w –weight weight 真实服务器的权值

-g –gatewaying 指定LVS 的工作模式为直接路由模式（也是LVS 默认的模式）
   使用-g选项,将模式改为DR模式

 -i, --ipip  Use ipip encapsulation (tunneling).
-i   --ipip 指定LVS 的工作模式为隧道模式

 -m, --masquerading  Use masquerading 
              (network  access  translation,  or   NAT).
-m   --masquerading 指定LVS 的工作模式为NAT 模式

-n   --numeric  输出IP 地址和端口的数字形式
 -n, --numeric   数字输出。
             IP地址和端口号将以数字格式打印，
                 而不是分别作为主机名和服务打印，这是默认值。












[root@V11 ~]# ipvsadm  -L  #显示内核虚拟服务器表

IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn

[root@V11 ~]# ipvsadm  -l   #显示内核虚拟服务器表
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn

[root@V11 ~]# 

  -A, --add-service
        添加虚拟服务。服务地址由
三重：IP地址、端口号和协议。或者，虚拟
服务可以由防火墙标记定义。

 -A  --add-service 在内核的虚拟服务器表中添加一条新的虚拟服务器记录。
                      也就是增加一台新的虚拟服务器。
-t –tcp-service service-address 说明虚拟服务器提供的是tcp 的服务
-s –scheduler scheduler 使用的调度算法

  -s  wrr  加权轮叫（Weight RR）
　 调度器通过“加权轮叫”调度算法
根据真实服务器的不同处理能力来调度访问请求。
这样可以保证处理能力强的服务器处理更多的访问流量。
调度器可以自动问询真实服务器的负载情况,
并动态地调整其权值。

1）创建LVS虚拟集群服务器(算法为加权轮询:wrr)
[root@V11 ~]# ipvsadm -A -t   192.168.0.11:80  -s  wrr
[root@V11 ~]# echo  $?
0
/**********************

修改集群服务器设置(修改调度器算法,将加权轮询修改为轮询)
 @proxy ~] # ipvsadm -E -t 192.168.0.11:80 -s  rr
***********************/

-a –add-server 在内核虚拟服务器表的一条记录里添加一条新的真实服务器

-r –real-server server-address 真实的服务器[Real-Server:port]

-w –weight weight 真实服务器的权值

-m   --masquerading 指定LVS 的工作模式为NAT 模式

-n   --numeric  输出IP 地址和端口的数字形式

2)添加真实服务器
--------------------------------------------添加realserver----------------

[root@V11 ~]# ipvsadm  -a  -t 192.168.0.11:80  -r 192.168.1.12 -w 1 -m
[root@V11 ~]# echo  $?
0
[root@V11 ~]# ipvsadm  -a  -t 192.168.0.11:80  -r 192.168.1.13 -w 2 -m
[root@V11 ~]# echo  $?
0

/******************
-e  --edit-server 编辑一条虚拟服务器记录中的某条真实服务器记录
修改read server(使用-g选项,将模式改为DR模式)
 @proxy ~] # ipvsadm -e -t 192.168.0.11:80 -r 192.168.1.12 -g
***********/

---------  ipvsadm  -L|-l –list 显示内核虚拟服务器表 查看LVS状态, 查看规则列表---------

[root@V11 ~]# ipvsadm   -Ln      #-n 输出IP 地址和端口的数字形式

IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.0.11:80 wrr
  -> 192.168.1.12:80              Masq    1      0          0         
  -> 192.168.1.13:80              Masq    2      0          0  
       
[root@V11 ~]# 
/************
-S, --save   将Linux虚拟服务器规则以 -R_--restore可以读取的格式  保存虚拟服务器规则，
                 输出为-R 选项可读的格式
 ipvsadm -S [-n]
 永久保存所有规则
   @proxy ~] # ipvsadm --save -n > /etc/sysconfig/ipvsadm
   # ipvsadm   -S    > /etc/sysconfig/ipvsadm
或者# ipvsadm-save  -n > /etc/sysconfig/ipvsadm

 清空所有规则
   @proxy ~] # ipvsadm -C

  载入保存在文件中的规则
# ipvsadm -R < /path/from/somefile

*************/
[root@V11 ~]# grep  -Env  "^#|^$"  /etc/sysconfig/ipvsadm-config #保存规则的配置文件
6:IPVS_MODULES_UNLOAD="yes"  模块卸载
12:IPVS_SAVE_ON_STOP="no"    在 ...停止时保存
18:IPVS_SAVE_ON_RESTART="no"  IPVS_重新启动时保存
23:IPVS_STATUS_NUMERIC="yes"  数字状态

[root@V11 ~]# vim  /etc/sysconfig/ipvsadm-config
[root@V11 ~]# grep  -Env  "^#|^$"  /etc/sysconfig/ipvsadm-config
6:IPVS_MODULES_UNLOAD="yes"
12:IPVS_SAVE_ON_STOP="yes"      #  在 ...停止时保存
18:IPVS_SAVE_ON_RESTART="yes"   #  IPVS_重新启动时保存
23:IPVS_STATUS_NUMERIC="yes"

--------------------------------------------   保存规则 ------------

[root@V11 ~]# ipvsadm-save  -n  > /etc/sysconfig/ipvsadm-save
[root@V11 ~]# cat   /etc/sysconfig/ipvsadm-save
-A -t 192.168.0.11:80 -s wrr
-a -t 192.168.0.11:80 -r 192.168.1.12:80 -m -w 1
-a -t 192.168.0.11:80 -r 192.168.1.13:80 -m -w 2

[root@V11 ~]# grep  -Env  "^#|^$"  /etc/sysconfig/ipvsadm-config
6:IPVS_MODULES_UNLOAD="yes"
12:IPVS_SAVE_ON_STOP="yes"
18:IPVS_SAVE_ON_RESTART="yes"
23:IPVS_STATUS_NUMERIC="yes"

[root@V11 ~]# ls  /etc/sysconfig/ipvsadm*
/etc/sysconfig/ipvsadm-config  /etc/sysconfig/ipvsadm-save

 ipvsadm -S [-n]
 永久保存所有规则
   @proxy ~] # ipvsadm --save -n > /etc/sysconfig/ipvsadm
   # ipvsadm -S    > /etc/sysconfig/ipvsadm
或者#s ipvsadm-save > /etc/sysconfig/ipvsadm

 
[root@V11 ~]# ipvsadm  -C   # 清空所有规则

[root@V11 ~]# ipvsadm  -Ln   #查看
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn

---------   -R  载入保存在文件中的规则 ----------------------

[root@V11 ~]# ipvsadm  -R  < /etc/sysconfig/ipvsadm-save 
[root@V11 ~]# ipvsadm  -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.0.11:80 wrr
  -> 192.168.1.12:80              Masq    1      0          0         
  -> 192.168.1.13:80              Masq    2      0          0 
      
[root@V11 ~]# systemctl  enable  ipvsadm.service 

Created symlink from /etc/systemd/system/multi-user.target.wants/ipvsadm.service to /usr/lib/systemd/system/ipvsadm.service.

[root@V11 ~]# ipvsadm  -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.0.11:80 wrr
  -> 192.168.1.12:80              Masq    1      0          0         
  -> 192.168.1.13:80              Masq    2      0          0  
       
[root@V11 ~]# ipvsadm  -Ln  --stats
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes
  -> RemoteAddress:Port
TCP  192.168.0.11:80                    15       90       60     6990     8345
  -> 192.168.1.12:80                     5       30       20     2330     2785
  -> 192.168.1.13:80                    10       60       40     4660     5560

[root@V11 ~]# systemctl status ipvsadm.service

● ipvsadm.service - Initialise the Linux Virtual Server
   Loaded: loaded (/usr/lib/systemd/system/ipvsadm.service; enabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since 五 2019-03-08 14:05:27 CST; 2min 50s ago
 Main PID: 4982 (code=exited, status=1/FAILURE)

3月 08 14:05:27 V11 systemd[1]: Starting Initialise the Linux Virtual Server...
3月 08 14:05:27 V11 bash[4982]: /bin/bash: /etc/sysconfig/ipvsadm: 没有那个文件或目录
......................... 提示没有  /etc/sysconfig/ipvsadm (默认保存规则的文件)


-------------------   /etc/sysconfig/ipvsadm (默认保存规则的文件) -------------

[root@V11 ~]# cp  /etc/sysconfig/ipvsadm-save   /etc/sysconfig/ipvsadm

[root@V11 ~]# ls  /etc/sysconfig/ipvsadm*
/etc/sysconfig/ipvsadm  /etc/sysconfig/ipvsadm-config  /etc/sysconfig/ipvsadm-save

[root@V11 ~]# systemctl  restart  ipvsadm
[root@V11 ~]# systemctl  is-active  ipvsadm
active

[root@V11 ~]# systemctl  is-enabled  ipvsadm
enabled



集群对外公网IP地址为192.168.0.11
调度器内网IP地址为192.168.1.11
真实Web服务器地址分别为192.168.1.12 、 192.168.1.13
使用加权轮询调度算法,真实服务器权重分别为1和2


部署LVS-NAT集群

client  V10  192.168.0.10
proxy   V11  192.168.0.11  192.168.1.11
web1    V12  192.168.1.12
web2    V13  192.168.1.13

[root@V12 ~]# vim  /etc/sysconfig/network-scripts/ifcfg-eth1
[root@V12 ~]# tail  -5  /etc/sysconfig/network-scripts/ifcfg-eth1
BOOTPROTO="static"
IPADDR="192.168.1.12"
NETMASK="255.255.255.0"
#GATEWAY="192.168.0.254"
GATEWAY="192.168.1.11"
[root@V12 ~]# systemctl  restart  network  && systemctl is-enabled  network
network.service is not a native service, redirecting to /sbin/chkconfig.
Executing /sbin/chkconfig network --level=5
enabled

[root@V12 ~]# ifdown  eth0
[root@V12 ~]# ifdown  eth2
[root@V12 ~]# ifconfig  |grep  -A1 flags=
eth1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.1.12  netmask 255.255.255.0  broadcast 192.168.1.255
--
lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
--
virbr0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        inet 192.168.122.1  netmask 255.255.255.0  broadcast 192.168.122.255

[root@V12 ~]# route  -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.1.11    0.0.0.0         UG    0      0        0 eth1
169.254.0.0     0.0.0.0         255.255.0.0     U     1003   0        0 eth1
192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 eth1
192.168.122.0   0.0.0.0         255.255.255.0   U     0      0        0 virbr0

[root@V12 ~]# service network status
已配置设备：
lo eth0 eth1 eth2
当前活跃设备：
lo eth1 virbr0
[root@V12 ~]# service network --help
Usage: service < option > | --status-all | [ service_name [ command | --full-restart ] ]


[root@V12 ~]# rpm  -q  elinks  httpd
elinks-0.12-0.36.pre6.el7.x86_64
httpd-2.4.6-67.el7.x86_64
[root@V12 ~]# echo   "web1    V12  192.168.1.12" > /var/www/html/index.html
[root@V12 ~]# systemctl  restart httpd  && systemctl  is-enabled  httpd
enabled
[root@V12 ~]# elinks  -dump  192.168.1.12
   web1 V12 192.168.1.12
[root@V12 ~]# 














部署LVS-NAT集群

client  V10  192.168.0.10
proxy   V11  192.168.0.11  192.168.1.11
web1    V12  192.168.1.12
web2    V13  192.168.1.13

[root@V13 ~]# ifdown  eth0
[root@V13 ~]# ifdown  eth2
[root@V13 ~]# ifconfig |grep -A1  flags=
eth1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.1.13  netmask 255.255.255.0  broadcast 192.168.1.255
--
lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
--
virbr0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        inet 192.168.122.1  netmask 255.255.255.0  broadcast 192.168.122.255

[root@V13 ~]# route  add  default  gw  192.168.1.11
[root@V13 ~]# route  -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.1.11    0.0.0.0         UG    0      0        0 eth1
169.254.0.0     0.0.0.0         255.255.0.0     U     1003   0        0 eth1
192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 eth1
192.168.122.0   0.0.0.0         255.255.255.0   U     0      0        0 virbr0

[root@V13 ~]# rpm  -q  elinks  httpd
elinks-0.12-0.36.pre6.el7.x86_64
httpd-2.4.6-67.el7.x86_64
[root@V13 ~]#  echo   "web2   V13  192.168.1.13" > /var/www/html/index.html
[root@V13 ~]#  systemctl  restart httpd  && systemctl  is-enabled  httpd
enabled
[root@V13 ~]# elinks  -dump  192.168.1.13
   web2 V13 192.168.1.13
[root@V13 ~]# 


[root@V11 ~]# type  watch  
watch 是 /usr/bin/watch
-------------------------------- 动态 监视 lvs 连接访问量 -------------

[root@V11 ~]# watch  -n 2  ipvsadm  -Ln  --stats
.............
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes
  -> RemoteAddress:Port
TCP  192.168.0.11:80                    15	 90	  60     6990     8345
  -> 192.168.1.12:80                     5	 30	  20     2330     2785
  -> 192.168.1.13:80                    10	 60	  40     4660     5560

^C

[root@host54 ~]# ipvsadm  -A  -t  192.168.4.253:80  -s  rr
[root@host54 ~]# ipvsadm  -a  -t  192.168.4.253:80  -r  192.168.4.52:80 -g
[root@host54 ~]# ipvsadm  -a  -t  192.168.4.253:80  -r  192.168.4.53:80 
[root@host54 ~]# ipvsadm  -S   #保存所有规则
[root@host54 ~]# ipvsadm-save  -n  #保存所有规则

-g –gatewaying 指定LVS 的工作模式为直接路由模式（也是LVS 默认的模式）
     使用-g选项,将模式改为DR模式

[root@host52 conf]# tail -5 /etc/rc.local
touch /var/lock/subsys/local
echo 1 >/proc/sys/net/ipv4/conf/lo/arp_ignore
echo 2 >/proc/sys/net/ipv4/conf/lo/arp_announce 
echo 1 >/proc/sys/net/ipv4/conf/all/arp_ignore 
echo 2 >/proc/sys/net/ipv4/conf/all/arp_announce
 

arp_ignore和arp_announce介绍

　　arp_ignore和arp_announce参数都和ARP协议相关，
主要用于控制系统返回arp响应和发送arp请求时的动作。
这两个参数很重要，
特别是在LVS的DR场景下，
它们的配置直接影响到DR转发是否正常。





Linux内核参数之arp_ignore和arp_announce

一、arp_ignore和arp_announce介绍

　　arp_ignore和arp_announce参数都和ARP协议相关，
主要用于控制系统返回arp响应和发送arp请求时的动作。
这两个参数很重要，
特别是在LVS的DR场景下，
它们的配置直接影响到DR转发是否正常。
首先看一下Linux内核文档中对于它们的描述：
 
　　arp_ignore参数的作用是控制系统在收到外部的arp请求时，是否要返回arp响应。
　　arp_ignore参数常用的取值主要有0，1，2，3~8较少用到：
0：响应任意网卡上接收到的对本机IP地址的arp请求（包括环回网卡上的地址），
而不管该目的IP是否在接收网卡上。
1：只响应目的IP地址为接收网卡上的本地地址的arp请求。
2：只响应目的IP地址为接收网卡上的本地地址的arp请求，
并且arp请求的源IP必须和接收网卡同网段。
3：如果ARP请求数据包所请求的IP地址对应的本地地址其作用域（scope）为主机（host），
则不回应ARP响应数据包，如果作用域为全局（global）或链路（link），
则回应ARP响应数据包。
4~7：保留未使用
8：不回应所有的arp请求
　　sysctl.conf中包含all和eth/lo（具体网卡）的arp_ignore参数，取其中较大的值生效。
 
　　arp_announce的作用是控制系统在对外发送arp请求时，
如何选择arp请求数据包的源IP地址。
（比如系统准备通过网卡发送一个数据包a，这时数据包a的源IP和目的IP一般都是知道的，
而根据目的IP查询路由表，发送网卡也是确定的，故源MAC地址也是知道的，
这时就差确定目的MAC地址了。
而想要获取目的IP对应的目的MAC地址，就需要发送arp请求。
arp请求的目的IP自然就是想要获取其MAC地址的IP，而arp请求的源IP是什么呢？
 可能第一反应会以为肯定是数据包a的源IP地址，但是这个也不是一定的，
arp请求的源IP是可以选择的，控制这个地址如何选择就是arp_announce的作用）
　　arp_announce参数常用的取值有0，1，2。
0：允许使用任意网卡上的IP地址作为arp请求的源IP，通常就是使用数据包a的源IP。
1：尽量避免使用不属于该发送网卡子网的本地地址作为发送arp请求的源IP地址。
2：忽略IP数据包的源IP地址，选择该发送网卡上最合适的本地地址作为arp请求的源IP地址。
　　sysctl.conf中包含all和eth/lo（具体网卡）的arp_ignore参数，取其中较大的值生效。
二、arp_ignore和arp_announce参数示例

 
（1）当arp_ignore参数配置为0时，eth1网卡上收到目的IP为环回网卡IP的arp请求，
但是eth1也会返回arp响应，把自己的mac地址告诉对端。



（2）当arp_ignore参数配置为1时，eth1网卡上收到目的IP为环回网卡IP的arp请求，
发现请求的IP不是自己网卡上的IP，不会回arp响应。


（3）当arp_announce参数配置为0时，系统要发送的IP包源地址为eth1的地址，
IP包目的地址根据路由表查询判断需要从eth2网卡发出，这时会先从eth2网卡发起一个arp请求，
用于获取目的IP地址的MAC地址。
该arp请求的源MAC自然是eth2网卡的MAC地址，但是源IP地址会选择eth1网卡的地址。

 

（4）当arp_announce参数配置为2时，eth2网卡发起arp请求时，
源IP地址会选择eth2网卡自身的IP地址。

 

三、arp_ignore和arp_announce参数在DR模式下的作用

　　1. arp_ignore

　　因为DR模式下，每个真实服务器节点都要在环回网卡上绑定虚拟服务IP。
这时候，如果客户端对于虚拟服务IP的arp请求广播到了各个真实服务器节点，
如果arp_ignore参数配置为0，则各个真实服务器节点都会响应该arp请求，
此时客户端就无法正确获取LVS节点上正确的虚拟服务IP所在网卡的MAC地址。
假如某个真实服务器节点A的网卡eth1响应了该arp请求，
客户端把A节点的eth1网卡的MAC地址误认为是LVS节点的虚拟服务IP所在网卡的MAC，
从而将业务请求消息直接发到了A节点的eth1网卡。
这时候虽然因为A节点在环回网卡上也绑定了虚拟服务IP，
所以A节点也能正常处理请求，业务暂时不会受到影响。
但时此时由于客户端请求没有发到LVS的虚拟服务IP上，
所以LVS的负载均衡能力没有生效。
造成的后果就是，A节点一直在单节点运行，业务量过大时可能会出现性能瓶颈。

　　所以DR模式下要求arp_ignore参数要求配置为1。

　　2. arp_announce

 　　每个机器或者交换机中都有一张arp表，
该表用于存储对端通信节点IP地址和MAC地址的对应关系。当收到一个未知IP地址的arp请求，
就会再本机的arp表中新增对端的IP和MAC记录；
当收到一个已知IP地址（arp表中已有记录的地址）的arp请求，
则会根据arp请求中的源MAC刷新自己的arp表。
　　如果arp_announce参数配置为0，则网卡在发送arp请求时，
可能选择的源IP地址并不是该网卡自身的IP地址，
这时候收到该arp请求的其他节点或者交换机上的arp表中记录的该网卡IP和MAC的对应关系就不正确，
可能会引发一些未知的网络问题，存在安全隐患。
　　所以DR模式下要求arp_announce参数要求配置为2。
四、arp_ignore和arp_announce参数的配置方法

arp_ignore和arp_announce参数分别有all,default,lo,eth1,eth2...等对应不同网卡的具体参数。
当all和具体网卡的参数值不一致时，取较大值生效。

一般只需修改all和某个具体网卡的参数即可（取决于你需要修改哪个网卡）。
下面以修改lo网卡为例：

 1. 修改/etc/sysctl.conf文件，然后sysctl -p刷新到内存。

 net.ipv4.conf.all.arp_ignore=1
 net.ipv4.conf.lo.arp_ignore=1
 net.ipv4.conf.all.arp_announce=2
 net.ipv4.conf.lo.arp_announce=2
 2. 使用sysctl -w直接写入内存：

 sysctl -w net.ipv4.conf.all.arp_ignore=1
 sysctl -w net.ipv4.conf.lo.arp_ignore=1
 sysctl -w net.ipv4.conf.all.arp_announce=2
 sysctl -w net.ipv4.conf.lo.arp_announce=2
 3. 修改/proc文件系统：

 echo "1">/proc/sys/net/ipv4/conf/all/arp_ignore
 echo "1">/proc/sys/net/ipv4/conf/lo/arp_ignore
 echo "2">/proc/sys/net/ipv4/conf/all/arp_announce
 echo "2">/proc/sys/net/ipv4/conf/lo/arp_announce


https://www.cnblogs.com/lipengxiang2009/p/7451050.html








